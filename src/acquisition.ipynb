{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "### Table of Contents\n",
    "- [Requirements](#requirements)\n",
    "- [Obtaining Historical Yield Data](#obtaining-historical-yield-data)\n",
    "- [Obtaining Historical Price Received Data](#obtaining-historical-price-received-data)\n",
    "- [Obtaining Historical Weather Data](#obtaining-historical-weather-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Historical Yield Data\n",
    "#### (Grain & Silage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the access key for the USDA NASS API\n",
    "\n",
    "with open('../keys/api_key.txt', 'r') as file:\n",
    "    usda_nass_key = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment of variables for later use\n",
    "\n",
    "states = ['ILLINOIS', 'INDIANA', 'IOWA', 'MINNESOTA', 'MISSOURI', 'NEBRASKA']\n",
    "\n",
    "parameters = {\n",
    "    'key': usda_nass_key,\n",
    "    'source_desc': 'SURVEY',\n",
    "    'sector_desc': 'CROPS',\n",
    "    'group_desc': 'FIELD CROPS',\n",
    "    'commodity_desc': 'CORN',\n",
    "    'statisticcat_desc': 'YIELD',\n",
    "    'agg_level_desc': 'STATE',\n",
    "    'format': 'csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoding query parameters according to URL standards\n",
    "encoded_parameters = urllib.parse.urlencode(parameters)\n",
    "\n",
    "## establishing the URL base for the query\n",
    "base_url = 'https://quickstats.nass.usda.gov/api/api_GET/'\n",
    "\n",
    "## constructing the API query from the URL base and the URL encoded parameters\n",
    "query = base_url+'?'+encoded_parameters\n",
    "\n",
    "## fetching the query response\n",
    "response = requests.get(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as yield_raw.csv\n"
     ]
    }
   ],
   "source": [
    "## if the query was successful:\n",
    "if response.status_code == 200:\n",
    "\n",
    "    ### convert query response to text\n",
    "    data = StringIO(response.text)\n",
    "\n",
    "    ### convert response text to a pandas dataframe\n",
    "    yield_raw = pd.read_csv(data)\n",
    "\n",
    "    ### narrow the obtained dataset to only the necessary columns to reduce storage waste\n",
    "    yield_raw = yield_raw[['year', 'util_practice_desc', 'state_name', 'reference_period_desc', 'Value']]\n",
    "\n",
    "    ### we are only interested in the yearly recorded yield, not forecasts or monthly estimates\n",
    "    yield_raw = yield_raw[yield_raw['reference_period_desc'] == 'YEAR']\n",
    "\n",
    "    ### save the dataframe as a local CSV\n",
    "    yield_raw.to_csv('../data/raw/yield_raw.csv', index=False) # file path appears as `data/raw/yield_raw.csv` in `acquisition.py`\n",
    "    print('Data saved as yield_raw.csv')\n",
    "\n",
    "## error handling\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Historical Price Received Data\n",
    "#### (By Month as well as Marketing Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the historical price received data is too large to be fetched in one query, we fetch each state's data separately\n",
    "for i, state in enumerate(states):\n",
    "\n",
    "    ## assignment of variables for later use\n",
    "    parameters = {\n",
    "        'key': usda_nass_key,\n",
    "        'source_desc': 'SURVEY',\n",
    "        'sector_desc': 'CROPS',\n",
    "        'group_desc': 'FIELD CROPS',\n",
    "        'commodity_desc': 'CORN',\n",
    "        'statisticcat_desc': 'PRICE RECEIVED',\n",
    "        'agg_level_desc': 'STATE',\n",
    "        'state_name':state,\n",
    "        'format': 'csv'\n",
    "    }\n",
    "\n",
    "    ## encoding query parameters according to URL standards\n",
    "    encoded_parameters = urllib.parse.urlencode(parameters)\n",
    "\n",
    "    ## establishing the URL base for the query\n",
    "    base_url = 'https://quickstats.nass.usda.gov/api/api_GET/'\n",
    "\n",
    "    ## constructing the API query from the URL base and the URL encoded parameters\n",
    "    query = base_url+'?'+encoded_parameters\n",
    "\n",
    "    ## fetching the query response\n",
    "    response = requests.get(query)\n",
    "\n",
    "    ## if the query was successful:\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        ### for the first state, ILLINOIS, create the `price_received_raw` dataframe\n",
    "        if i == 0:\n",
    "            data = StringIO(response.text)\n",
    "            price_received_raw = pd.read_csv(data)\n",
    "            price_received_raw = price_received_raw[['year', 'state_name', 'reference_period_desc', 'Value']]\n",
    "\n",
    "        ### for every state thereafter, create a temporary dataframe with the 'next' state's data and merge it with the `price_received_raw` dataframe\n",
    "        else:\n",
    "            data = StringIO(response.text)\n",
    "            temp_df = pd.read_csv(data)\n",
    "            temp_df = temp_df[['year', 'state_name', 'reference_period_desc', 'Value']]\n",
    "            price_received_raw = pd.concat([price_received_raw, temp_df])\n",
    "\n",
    "    ## error handling\n",
    "    else:\n",
    "        print(f'Request failed for {state} with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as price_received_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# save the dataframe as a local CSV\n",
    "price_received_raw.to_csv('../data/raw/price_received_raw.csv', index=False) # file path appears as `data/raw/price_received_raw.csv` in `acquisition.py`\n",
    "print('Data saved as price_received_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Historical Weather Data\n",
    "#### (Average/Maximum/Minimum Temperature, Total Precipitation, & PDSI by Month from April through November)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assignment of variables for later use\n",
    "months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'semptember', 'october', 'november', 'december']\n",
    "state_numbers = [11, 12, 13, 21, 23, 25]\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "\n",
    "## iterate through each state, variable, & month combination\n",
    "for state_number in state_numbers:\n",
    "    for variable in ['tavg', 'tmax', 'tmin', 'pcp', 'pdsi']:\n",
    "        for month_number in range(4, 12):\n",
    "\n",
    "            ### skip the appropriate number of rows when reading the CSV\n",
    "            skiprows = 3 if variable == 'pdsi' else 4\n",
    "\n",
    "            ### construct the appropriate CSV link\n",
    "            link = f'https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series/{state_number}/{variable}/12/{month_number}/1895-2024.csv?base_prd=true&begbaseyear=1895&endbaseyear=2000'\n",
    "\n",
    "            ### for the first month (April) of each state's iteration:\n",
    "            if count1 == 0:\n",
    "\n",
    "                #### create the `state_weather` dataframe\n",
    "                state_weather = pd.read_csv(link, skiprows=skiprows)\n",
    "\n",
    "                #### drop unnecessary columns\n",
    "                state_weather.drop(\n",
    "                    columns=state_weather.columns[2:],\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "                #### rename the value column to its appropriate name\n",
    "                state_weather.rename(\n",
    "                    columns={state_weather.columns[1]:f'{months[month_number]}_{variable}'},\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "                #### convert the `Date` (YYYYMM) column to YYYY format for later merging\n",
    "                state_weather['Date'] = state_weather['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "                count1 += 1\n",
    "            \n",
    "            ### for every month thereafter for each state's iteration:\n",
    "            else:\n",
    "\n",
    "                #### create a temporary dataframe to store this month's variable's data\n",
    "                temp = pd.read_csv(link, skiprows=skiprows)\n",
    "\n",
    "                #### drop unnecessary columns\n",
    "                temp.drop(\n",
    "                    columns=temp.columns[2:],\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "                #### rename the value column to its appropriate name\n",
    "                temp.rename(\n",
    "                    columns={temp.columns[1]:f'{months[month_number]}_{variable}'},\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "                #### convert the `Date` (YYYYMM) column to YYYY format for later merging\n",
    "                temp['Date'] = temp['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "\n",
    "                #### merge the temporary dataframe with the `state_weather` dataframe\n",
    "                state_weather = state_weather.merge(\n",
    "                    temp,\n",
    "                    on='Date',\n",
    "                    how='outer'\n",
    "                )\n",
    "                count1 += 1\n",
    "        \n",
    "    ### assign the appropriate state name for each record in the new column, `state`\n",
    "    state_weather['state'] = states[state_numbers.index(state_number)]\n",
    "\n",
    "    ### for the first state, ILLINOIS, create the `weather_raw` dataframe\n",
    "    if count2 == 0:\n",
    "        weather_raw = state_weather.copy()\n",
    "        count2 += 1\n",
    "        count1 = 0\n",
    "\n",
    "    ### for every state thereafter, concatenate that state's dataframe with the `weather_raw` dataframe\n",
    "    else:\n",
    "        weather_raw = pd.concat([weather_raw, weather_raw])\n",
    "        count2 += 1\n",
    "        count1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as weather_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# save the dataframe as a local CSV\n",
    "weather_raw.to_csv('../data/raw/weather_raw.csv', index=False) # file path appears as `data/raw/weather_raw.csv` in `acquisition.py`\n",
    "print('Data saved as weather_raw.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
